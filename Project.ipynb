{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburbs = []\n",
    "subs = open('areas.txt', 'r')\n",
    "for line in subs:\n",
    "    line = line.replace('(', '')\n",
    "    line = line.replace(')', '')\n",
    "    line = line.strip()\n",
    "    suburbs.append(line)\n",
    "subs.close\n",
    "print(suburbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for i in range(0, len(suburbs)):\n",
    "  filenames.append(\"./Reviews/\"+str(suburbs[i])+\"/reviews.csv\")\n",
    "  # filenames = [\n",
    "  #             \"./Reviews/ANUJA HOSPITAL/reviews.csv\",\n",
    "  #           ]\n",
    "\n",
    "# merging two csv files\n",
    "df = pd.concat(\n",
    "    map(pd.read_csv, filenames), ignore_index=True)\n",
    "\n",
    "print(len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_remove_translated = []\n",
    "\n",
    "reviews_dict = df.to_dict('list')\n",
    "\n",
    "for review in reviews_dict['Review']:\n",
    "  review_sep = str(review).split(\"(Translated by Google) \")\n",
    "\n",
    "  if review_sep[0] == \"\":\n",
    "    review_sep = (\"\".join(review_sep)).split(\"(Original)\")\n",
    "    review_sep = review_sep[0]\n",
    "    review = \"\".join(review_sep)\n",
    "  \n",
    "  review_remove_translated.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_dict['Review'] = review_remove_translated\n",
    "df = pd.DataFrame(reviews_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review):\n",
    "    return re.sub('[^a-zA-Z]', ' ', review).lower()\n",
    "  \n",
    "df['cleaned_review'] = df['Review'].apply(lambda x: clean_review(str(x)))\n",
    "df['label'] = df['Rating'].map({\"1 star\":0, \"2 stars\":0, \"3 stars\":0, \"4 stars\":1, \"5 stars\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punct(review):\n",
    "    count = sum([1 for char in review if char in string.punctuation])\n",
    "    return round(count/(len(review) - review.count(\" \")), 3)*100\n",
    "  \n",
    "df['review_len'] = df['Review'].apply(lambda x: len(str(x)) - str(x).count(\" \"))\n",
    "df['punct'] = df['Review'].apply(lambda x: count_punct(str(x)))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_review(review):\n",
    "    tokenized_review = review.split()\n",
    "    return tokenized_review\n",
    "  \n",
    "df['tokens'] = df['cleaned_review'].apply(lambda x: tokenize_review(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_review(token_list):\n",
    "    return \" \".join([lemmatizer.lemmatize(token) for token in token_list if not token in set(all_stopwords)])\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "df['lemmatized_review'] = df['tokens'].apply(lambda x: lemmatize_review(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Shape of the dataset, and breakdown of the classes\n",
    "print(f\"Input data has { len(df) } rows and { len(df.columns) } columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of null in label: { df['Rating'].isnull().sum() }\")\n",
    "print(f\"Number of null in text: { df['Review'].isnull().sum() }\")\n",
    "sns.countplot(x='Rating', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['lemmatized_review', 'review_len', 'punct']]\n",
    "y = df['label']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_df = 0.5, min_df = 2) # ignore terms that occur in more than 50% documents and the ones that occur in less than 2\n",
    "tfidf_train = tfidf.fit_transform(X_train['lemmatized_review'])\n",
    "tfidf_test = tfidf.transform(X_test['lemmatized_review'])\n",
    "\n",
    "X_train_vect = pd.concat([X_train[['review_len', 'punct']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "X_test_vect = pd.concat([X_test[['review_len', 'punct']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
    "\n",
    "X_train_vect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vect, y_train)\n",
    "naive_bayes_pred = classifier.predict(X_test_vect)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, naive_bayes_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "class_label = [\"negative\", \"positive\"]\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, naive_bayes_pred), index=class_label, columns=class_label)\n",
    "sns.heatmap(df_cm, annot=True, fmt='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_vect, y_train)\n",
    "log_reg_pred = classifier.predict(X_test_vect)\n",
    "# Classification report\n",
    "print(classification_report(y_test, log_reg_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "class_label = [\"negative\", \"positive\"]\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, log_reg_pred), index=class_label, columns=class_label)\n",
    "sns.heatmap(df_cm, annot=True, fmt='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "# classifier.fit(X_train_vect, y_train)\n",
    "# svm_pred = classifier.predict(X_test_vect)\n",
    "# # Classification report\n",
    "# print(classification_report(y_test, svm_pred))\n",
    "\n",
    "# # Confusion Matrix\n",
    "# class_label = [\"negative\", \"positive\"]\n",
    "# df_cm = pd.DataFrame(confusion_matrix(y_test, svm_pred), index=class_label, columns=class_label)\n",
    "# sns.heatmap(df_cm, annot=True, fmt='d')\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.xlabel(\"Predicted Label\")\n",
    "# plt.ylabel(\"True Label\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5)\n",
    "classifier.fit(X_train_vect, y_train)\n",
    "knn_pred = classifier.predict(X_test_vect)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, knn_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "class_label = [\"negative\", \"positive\"]\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, knn_pred), index=class_label, columns=class_label)\n",
    "sns.heatmap(df_cm, annot=True, fmt='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# models = [\n",
    "#           MultinomialNB(),\n",
    "#           LogisticRegression(),\n",
    "#           SVC(kernel = 'linear'),\n",
    "#           KNeighborsClassifier(n_neighbors = 5),\n",
    "#          ]\n",
    "# names = [\"Naive Bayes\", \"Logistic Regression\", \"SVM\", \"KNN\"]\n",
    "# for model, name in zip(models, names):\n",
    "#     print(name)\n",
    "#     for score in [\"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
    "#         print(f\" {score} - {cross_val_score(model, X_train_vect, y_train, scoring=score, cv=10).mean()} \")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 10)\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# classifier = ExtraTreesClassifier(n_estimators=150, random_state=50)\n",
    "\n",
    "classifier.fit(tfidf_train, y_train)\n",
    "classifier.score(tfidf_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Bad\", \"Good\", \"I love the service, it's really good\", \"Worst\"]\n",
    "vect = tfidf.transform(data).toarray()\n",
    "\n",
    "my_pred = classifier.predict(vect)\n",
    "print(my_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_cv = cv.fit_transform(df['lemmatized_review']) # Fit the Data\n",
    "y_cv = df['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_cv, y_cv, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train_cv, y_train_cv)\n",
    "clf.score(X_test_cv, y_test_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Bad\", \"Worst service, don't go there\", \"Services are OK\", \"Good service\", \"The nurse is so kind\"]\n",
    "vect = cv.transform(data).toarray()\n",
    "\n",
    "my_prediction = clf.predict(vect)\n",
    "print(my_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = open(\"./Reviews/Sancheti Hospital/reviews.csv\", \"r\")\n",
    "txt = pointer.read()\n",
    "service = []\n",
    "#print(txt)\n",
    "\n",
    "def find_services(txt, services):\n",
    "    ortho = re.search(\"[a-z].*[oO]rtho*\", txt)\n",
    "    pedia = re.search(\"[a-z].*[pP]edia*\", txt)\n",
    "    cardio = re.search(\"[a-z].*[cC]ardio*\", txt)\n",
    "    derma = re.search(\"[a-z].*[dD]erma*\", txt)\n",
    "    endo = re.search(\"[a-z].*[eE]ndo*\", txt)\n",
    "    gastro = re.search(\"[a-z].*[gG]astro*\", txt)\n",
    "    nephro = re.search(\"[a-z].*[nN]ephro*\", txt)\n",
    "    neuro = re.search(\"[a-z].*[nN]euro*\", txt)\n",
    "    onco = re.search(\"[a-z].*[oO]nco*\", txt)\n",
    "    optha = re.search(\"[a-z].*[oO]ptha*\", txt)\n",
    "    vaccine = re.search(\"[a-z].*[vV]accin*\", txt)\n",
    "\n",
    "    if ortho:\n",
    "        services.append(\"orthopedic\")\n",
    "    if pedia:\n",
    "        services.append(\"pediatric\")\n",
    "    if cardio:\n",
    "        services.append(\"cardiology\")\n",
    "    if derma:\n",
    "        services.append(\"dermatology\")\n",
    "    if endo:\n",
    "        services.append(\"endocrinology\")\n",
    "    if gastro:\n",
    "        services.append(\"gastrology\")\n",
    "    if nephro:\n",
    "        services.append(\"nephrology\")\n",
    "    if neuro:\n",
    "        services.append(\"neurology\")\n",
    "    if onco:\n",
    "        services.append(\"oncology\")\n",
    "    if optha:\n",
    "        services.append(\"opthalogy\")\n",
    "    if vaccine:\n",
    "        services.append(\"vaccination\")\n",
    "\n",
    "    return services\n",
    "\n",
    "services = find_services(txt, service)\n",
    "print(services)\n",
    "\n",
    "pointer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.4491376\n",
    "#73.8760855\n",
    "\n",
    "lat = float(input(\"Enter your latitude: \"))\n",
    "lon = float(input(\"Enter your longitude: \"))\n",
    "\n",
    "ran = float(input(\"Enter the range(in km): \"))\n",
    "\n",
    "def testing():\n",
    "\n",
    "        change_per_deg_lat = 111.2\n",
    "        change_per_deg_long = 105.75\n",
    "\n",
    "        df = pd.read_csv(\"modified.csv\")\n",
    "\n",
    "        for row in df.itertuples():\n",
    "\n",
    "                x = abs(float(row[6]) - lon) * change_per_deg_long\n",
    "                y = abs(float(row[5]) - lat) * change_per_deg_lat\n",
    "                dist = (x**2 + y**2)**(1/2)\n",
    "\n",
    "                if dist <= ran:\n",
    "                        #print(row[1])\n",
    "\n",
    "                        if row[1] not in suburbs:\n",
    "                                continue\n",
    "                        process = read_csv(\"./Reviews/\"+str(row[1])+\"/reviews.csv\")\n",
    "                        pointer = open(\"./Reviews/\"+str(row[1])+\"/reviews.csv\", \"r\")\n",
    "                        txt = pointer.read()\n",
    "                        service = []\n",
    "\n",
    "                        services = find_services(txt, service)\n",
    "                        data = process['Review'].to_list()\n",
    "                        vect = cv.transform(data).toarray()\n",
    "\n",
    "                        my_prediction = clf.predict(vect)\n",
    "                        flag = \"Bad\"\n",
    "                        zeroes = len(my_prediction) - np.count_nonzero(my_prediction)\n",
    "                        # print(zeroes)\n",
    "                        # print(np.count_nonzero(my_prediction))\n",
    "                        if np.count_nonzero(my_prediction) > 3*zeroes:\n",
    "                                flag = \"Good\"\n",
    "                        if np.count_nonzero(my_prediction) > 4*(zeroes+1):\n",
    "                                flag = \"Excellent\"\n",
    "                        if np.count_nonzero(my_prediction) <= 3*zeroes:\n",
    "                                flag = \"Not good\"\n",
    "                        # print(my_prediction)\n",
    "                        \n",
    "                        sys.stdout.write(\"\\033[1;32m\")\n",
    "                        if flag == \"Not good\":\n",
    "                                sys.stdout.write(\"\\033[1;31m\")\n",
    "                                print(\"We do not recommend \"+str(row[1])+\" since it is\",flag,\"hospital\")\n",
    "                        elif flag == \"Good\":\n",
    "                                sys.stdout.write(\"\\033[1;35m\")\n",
    "                                print(str(row[1])+\" is a\",flag,\"hospital\")\n",
    "                        else:\n",
    "                                print(\"We recommend \"+str(row[1])+\" ,it is an\",flag,\"hospital\")\n",
    "                        sys.stdout.write(\"\\033[1;33m\")\n",
    "                        if len(services):\n",
    "                                print(\"Here, the specialities offered are \")\n",
    "                                print(services)\n",
    "\n",
    "                        print(\"\\n\")\n",
    "                        sys.stdout.write(\"\\033[0;0m\")\n",
    "\n",
    "        \n",
    "\n",
    "testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\n",
      "We recommend Sai Sneh Hospital and Diagnostic Center for oncology ,it is an Excellent hospital\n",
      "\u001b[1;33mHere, the specialities offered are \n",
      "['cardiologic', 'endocrinology', 'oncology']\n",
      "\u001b[0;0m\u001b[1;35m\u001b[1;32m\n",
      "Bharati Hospital is a Good hospital for oncology\n",
      "\u001b[1;33mHere, the specialities offered are \n",
      "['orthopedic', 'pediatric', 'dermatology', 'endocrinology', 'gastrology', 'neurology', 'oncology', 'vaccination']\n",
      "\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\n",
      "We recommend Dr. Masal Hospital for oncology ,it is an Excellent hospital\n",
      "\u001b[1;33mHere, the specialities offered are \n",
      "['orthopedic', 'pediatric', 'endocrinology', 'oncology']\n",
      "\u001b[0;0m\u001b[1;35m\n",
      "We recommend Satyanand Hospital - Best Elderly Care In Pune for oncology ,it is an Excellent hospital\n",
      "\u001b[1;33mHere, the specialities offered are \n",
      "['orthopedic', 'pediatric', 'endocrinology', 'oncology']\n",
      "\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;32m\n",
      "Sangam Hospital - Undri is a Good hospital for oncology\n",
      "\u001b[1;33mHere, the specialities offered are \n",
      "['pediatric', 'endocrinology', 'oncology', 'vaccination']\n",
      "\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;33m\u001b[0;0m\u001b[1;35m\u001b[1;32m\n",
      "Vighnaharta Hospital is a Good hospital for oncology\n",
      "\u001b[1;33mHere, the specialities offered are \n",
      "['endocrinology', 'oncology']\n",
      "\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "#18.4491376\n",
    "#73.8760855\n",
    "\n",
    "lat = float(input(\"Enter your latitude: \"))\n",
    "lon = float(input(\"Enter your longitude: \"))\n",
    "\n",
    "ran = float(input(\"Enter the range(in km): \"))\n",
    "speciality = input(\"Enter the speciality you are in search for\")\n",
    "\n",
    "Hospitals = {}\n",
    "specialist = {}\n",
    "\n",
    "def recommend_hospital():\n",
    "\n",
    "        change_per_deg_lat = 111.2\n",
    "        change_per_deg_long = 105.75\n",
    "\n",
    "        df = pd.read_csv(\"modified.csv\")\n",
    "\n",
    "        for row in df.itertuples():\n",
    "\n",
    "                x = abs(float(row[6]) - lon) * change_per_deg_long\n",
    "                y = abs(float(row[5]) - lat) * change_per_deg_lat\n",
    "                dist = (x**2 + y**2)**(1/2)\n",
    "\n",
    "                if dist <= ran:\n",
    "                        #print(row[1])\n",
    "\n",
    "                        if row[1] not in suburbs:\n",
    "                                continue\n",
    "                        process = read_csv(\"./Reviews/\"+str(row[1])+\"/reviews.csv\")\n",
    "                        pointer = open(\"./Reviews/\"+str(row[1])+\"/reviews.csv\", \"r\")\n",
    "                        txt = pointer.read()\n",
    "                        service = []\n",
    "\n",
    "                        services = find_services(txt, service)\n",
    "                        data = process['Review'].to_list()\n",
    "                        vect = cv.transform(data).toarray()\n",
    "\n",
    "                        my_prediction = clf.predict(vect)\n",
    "                        flag = \"Bad\"\n",
    "                        good_reviews = np.count_nonzero(my_prediction) / len(my_prediction)\n",
    "                        Hospitals[str(row[1])] = good_reviews\n",
    "\n",
    "                        if speciality in services:\n",
    "                                specialist[str(row[1])] = speciality\n",
    "                        \n",
    "                        \n",
    "                        # print(good_reviews)\n",
    "                        # print(np.count_nonzero(my_prediction))\n",
    "                        # print(speciality)\n",
    "                        \n",
    "                        if good_reviews > 0.5:\n",
    "                                flag = \"Good\"\n",
    "                        if (good_reviews > 0.75) & (len(my_prediction) > 100):\n",
    "                                flag = \"Excellent\"\n",
    "                        if good_reviews <= 0.5:\n",
    "                                flag = \"Not good\"\n",
    "                        # print(my_prediction)\n",
    "                        \n",
    "                        sys.stdout.write(\"\\033[1;35m\")\n",
    "                        if flag == \"Good\":\n",
    "                                if speciality in services:\n",
    "                                        sys.stdout.write(\"\\033[1;32m\")\n",
    "                                        print(\"\\n\"+str(row[1])+\" is a\",flag,\"hospital for\",speciality)\n",
    "                        elif flag == \"Excellent\":\n",
    "                                if speciality in services:\n",
    "                                        print(\"\\nWe recommend \"+str(row[1])+\" for\",speciality,\",it is an\",flag,\"hospital\")\n",
    "                        sys.stdout.write(\"\\033[1;33m\")\n",
    "                        if speciality in services:\n",
    "                                print(\"Here, the specialities offered are \")\n",
    "                                print(services)\n",
    "\n",
    "                        sys.stdout.write(\"\\033[0;0m\")\n",
    "\n",
    "recommend_hospital()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = read_csv(\"./Reviews/Sancheti Hospital/reviews.csv\")\n",
    "data = process['Review'].to_list()\n",
    "vect = cv.transform(data).toarray()\n",
    "\n",
    "my_prediction = clf.predict(vect)\n",
    "flag = \"Bad\"\n",
    "zeroes = len(my_prediction) - np.count_nonzero(my_prediction)\n",
    "if np.count_nonzero(my_prediction) > zeroes:\n",
    "    flag = \"Good\"\n",
    "# print(my_prediction)\n",
    "# print(zeroes)\n",
    "# print(np.count_nonzero(my_prediction))\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hosp = recommend_hospital()\n",
    "#18.4491376\n",
    "#73.8760855\n",
    "print(list_hosp)\n",
    "for x in list_hosp:\n",
    "    print(x)\n",
    "    if x in suburbs:\n",
    "        print(\"Control reached here\\n\")\n",
    "        process = read_csv(\"./Reviews/\"+str(x)+\"/reviews.csv\")\n",
    "        data = process['Review'].to_list()\n",
    "        vect = cv.transform(data).toarray()\n",
    "\n",
    "        my_prediction = clf.predict(vect)\n",
    "        flag = \"Bad\"\n",
    "        zeroes = len(my_prediction) - np.count_nonzero(my_prediction)\n",
    "        if np.count_nonzero(my_prediction) > zeroes:\n",
    "            flag = \"Good\"\n",
    "        # print(my_prediction)\n",
    "        # print(zeroes)\n",
    "        # print(np.count_nonzero(my_prediction))\n",
    "        print(flag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
